### 神经网络

#### 1.感知机
##### 感知机图示
![感知器图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/感知机图示.png)  
* 组成 : 输入层 -> 输出层  
    其中输入层为信号,从输入层到输出层需要经过权重W的组合计算  


#### 2.神经网络
##### 神经网络图示
![神经网络图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/神经网络图示)

##### 神经网络的组成  
* 基本组成 : 输入层 -> 隐层 -> 输出层  
    1. 其中隐层对输入层的数据进行处理,各层之间需要经过权重W和输入的计算,以及不同函数的处理;  
    2. 注意神经网络的层数由具有权重的层数决定,有几层具有权重,即神经网络的层数为几层;  
    
* 计算过程   
![计算过程图示]()
    1. 组合权重W和偏置B求和(以两层为例) : a = W1 * x1 + W2 * x2 + b
    2. 利用激活函数求得结果 : y = h(a)  
    
* 激活函数  
    1. 阶跃函数
	1. 图示
	2. 函数表达式为：h(x) =   
	3. 在Python中的实现方式：      
    2. sigmoid函数 
	1. 图示
	2. 函数表达式为：h(x) = 1/( 1+exp(-x) )  
	3. 在Python中实现方式： 
	```python
		# 其中np为Numpy, x 可以为Numpy数组
		def sigmoid(x):
			return 1/(1+np.exp(-x))
	```  

    3. 二者区别  
	1. 共同点：宏观角度来看，形状相似，即当输入信号为重要信息时，
阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，
两者都输出较小的值；两者均为非线性函数，注意激活函数必须为非线性函数，否则神经网络分层就没有意义了；
	2. 区别： 平滑性不同；阶跃函数只能输出0/1，sigmoid能输出连续的实数； 
    4. ReLu函数：  
    	1. 图示：  
	2. 函数的表达式： 
	3. 在Python中的实现方式
* 神经网络的信号传递实现三层为例)  
    1. 符号说明  
    2. 输入层 -> 第一层  
    3. 第一层 -> 第二层   
    4. 第二层 -> 第三层  
    5. 第三层 -> 输出层  

* 利用神经网络解决实际问题: 字母识别(详见....)  
    1. 收集数据集  
    2. 推理过程  
    3. 实验结果       
    
#### 3.神经网络的"学习"

##### 从数据中学习  
##### 损失函数  
##### 数值微分  
##### 梯度  
##### 学习算法的实现  

#### 4.误差反向传播学习法
#### 5.卷积神经网络
#### 6.深度学习
