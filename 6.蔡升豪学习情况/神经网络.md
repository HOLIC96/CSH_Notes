### 神经网络

#### 1.感知机
##### 感知机图示
![感知器图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/感知机图示.png)  
* 组成 : 输入层 -> 输出层  
    其中输入层为信号,从输入层到输出层需要经过权重W的组合计算  


#### 2.神经网络
##### 神经网络图示
![神经网络图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/神经网络图示.png)

##### 神经网络的组成  
* 基本组成 : 输入层 -> 隐层 -> 输出层  
    1. 其中隐层对输入层的数据进行处理,各层之间需要经过权重W和输入的计算,以及不同函数的处理;  
    2. 注意神经网络的层数由具有权重的层数决定,有几层具有权重,即神经网络的层数为几层;  
    
* 计算过程   
![计算过程图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/激活函数计算过程.png)
    1. 组合权重W和偏置B求和(以两层为例) : a = W1 * x1 + W2 * x2 + b
    2. 利用激活函数求得结果 : y = h(a)  
    
* 激活函数: h(x)  
    1. 阶跃函数
	    1. 图示  
	    ![跃迁函数图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/跃迁函数.png)
	    2. 函数表达式为：h(x) =   
	    3. 在Python中的实现方式：      
    2. sigmoid函数 
	    1. 图示  
	    ![sigmoid函数](https://github.com/ZWtu19/Notes/blob/master/src/pics/sigmoid函数.png)
	    2. 函数表达式为：h(x) = 1/( 1+exp(-x) )  
	    3. 在Python中实现方式： 
        ```python
        # 其中np为Numpy, x 可以为Numpy数组
            import numpy as np
            def sigmoid(x):
                    return 1/(1+np.exp(-x))
         ```

    3. 二者异同之处:   
	    1. 共同点：宏观角度来看，形状相似，即当输入信号为重要信息时，
阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，
两者都输出较小的值；两者均为非线性函数，注意激活函数必须为非线性函数，否则神经网络分层就没有意义了；
	    2. 区别： 平滑性不同；阶跃函数只能输出0/1，sigmoid能输出连续的实数； 
    4. ReLu函数：  
    	1. 图示：  
	    2. 函数的表达式： 
	    3. 在Python中的实现方式  
	    
* 神经网络的信号传递实现三层为例)  
    1. 符号说明  
    ![权重符号说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/权重符号说明.png)    
    2. 输入层 -> 第一层  
        1. 表达式    
        a1 = w11 * x1 + w12 * x2 + b1;
        a2 = w21 * x1 + w22 * x2 + b2;  
        a3 = w31 * x1 + w22 * x2 + b3;    
        z1 = h(a1);  
        z2 = h(a2);  
        z3 = h(a3);  
        
        2. 图示说明  
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/输入层->第一层.png)
        3. 矩阵表示  
        A = XW + B  
        Z = h(A)  
        其中:  
        A = [a1, a2, a3];  
        X = [x1, x2];  
        B = [b1, b2, b3];  
        w = [[w11, w21, w31],
             [w12, w22, w32]]  
        4. 代码实现  
        ```python
            import numpy as np
            X = np.array([1.0, 0.5])
            W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) B1 = np.array([0.1, 0.2, 0.3])
            print(W1.shape) # (2, 3) print(X.shape) # (2,) print(B1.shape) # (3,)
            A1 = np.dot(X, W1) + B1
        ```  
    3. 第一层 -> 第二层  
        1. 表达式  
        a1' = w'11 * z1 + w'12 * z2 + w'13 * z3 + b'1;
        a2' = w'21 * z1 + w'22 * z2 + w'23 * z3 + b'2;      
        z1' = h'(a1);  
        z2' = h'(a2);  
         
        2. 图示说明
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/第一层->第二层.png)  
        2. 矩阵表示  
        A' = ZW' + B'  
        Z' = h'(A')
        其中:  
        A' = [a1', a2', a3'];  
        Z  = [z1, z2, z3];  
        B' = [b1', b2'];  
        W' = [[w11, w21],
             [w12, w22],
             [w13, w23]]  
          
        3. 代码实现   
        ```python
            W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
            B2 = np.array([0.1, 0.2])
            print(Z1.shape) # (3,) print(W2.shape) # (3, 2) print(B2.shape) # (2,)
            A2 = np.dot(Z1, W2) + B2 
            Z2 = sigmoid(A2)
        ```  
       
    4. 第二层 -> 输出层
        1. 表达式  
        a1'' = w'11 * z1' + w'12 * z2' b''1;  
        a2'' = w'21 * z1' + w'22 * z2' b''2;       
        y1'' = identity_function(a1'');  
        y2'' = identity_function(a2'');  
        
        2. 图示说明  
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/第二层->输出层.png)  
         
        3. 矩阵表示  
        A'' = ZW'' + B''  
        Z'' = identity_function(A'')
        其中:  
        A'' = [a1'', a2'', a3''];  
        Z''  = [z1', z2', z3'];  
        B'' = [b1'', b2''];  
        W'' = [[w11'', w21''],
             [w12'', w22''],
             [w13'', w23'']]  
         
        4. 代码实现  
        ```python
           def identity_function(x): return x
           W3 = np.array([[0.1, 0.3], [0.2, 0.4]])  
           B3 = np.array([0.1, 0.2])
           A3 = np.dot(Z2, W3) + B3
           Y = identity_function(A3) # 或者Y = A3                      

        ```
     
* 利用神经网络解决实际问题: 字母识别(详见....)  
    1. 收集数据集  
    从官网获取数据: http://yann.lecun.com/exdb/mnist/,下载数据  
        * train-images-idx3-ubyte.gz
        * train-labels-idx1-ubyte.gz
        * t10k-images-idx3-ubyte.gz
        * t10k-labels-idx1-ubyte.gz
        
    2. 程序实现  
    ```python
    # 导入模块
    import numpy as np
    from struct import unpack
    # 读取图片  
    def read_image(path):
      with open(path, 'rb') as f:
        magic, num, rows, cols = unpack('>4I', f.read(16))
        img = np.fromfile(f, dtype=np.uint8).reshape(num, 784)
      return img        
    # 读取标签文件  
    def read_label(path):
      with open(path, 'rb') as f:
          magic, num = unpack('>2I', f.read(8))
          lab = np.fromfile(f, dtype=np.uint8)
          return lab      
    
    # 数据处理
    # 图像的像素值正规化  
    def normalize_image(image):
      img = image.astype(np.float32) / 255.0
      return img    

    # 转换标签为one-hot编码
    def __one_hot_label(label):
        lab = np.zeros((label.size, 10))
        for i, row in enumerate(lab):
            row[label[i]] = 1
        return lab
    
    # mnist读取函数  
    def load_mnist(train_image_path, train_label_path, test_image_path, test_label_path, normalize=True, one_hot=True):
        '''读入MNIST数据集
        Parameters
        ----------
        normalize : 将图像的像素值正规化为0.0~1.0
        one_hot_label : 
            one_hot为True的情况下，标签作为one-hot数组返回
            one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组
        Returns
        ----------
        (训练图像, 训练标签), (测试图像, 测试标签)
        '''
        image = {
            'train' : __read_image(train_image_path),
            'test'  : __read_image(test_image_path)
        }
    
        label = {
            'train' : __read_label(train_label_path),
            'test'  : __read_label(test_label_path)
        }
        
        if normalize:
            for key in ('train', 'test'):
                image[key] = __normalize_image(image[key])
    
        if one_hot:
            for key in ('train', 'test'):
                label[key] = __one_hot_label(label[key])
    
        return (image['train'], label['train']), (image['test'], label['test'])

    ```      
    
#### 3.神经网络的"学习"
##### 从数据中学习  
* 数据驱动  
    1. 人工设计规则  
    2. 人工设置特征量  
    3. 机器从数据中学习  
    ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/数据驱动.png)  
    4. 利用数据驱动的优点：  
	对于所有的问题都可以用同样的流程来解决；例如图像识别问题，不管是识别猫狗或是人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题。
	
* 训练数据和测试数据  
    1. 在机器学习中数据分类为训练数据和测试数据两部分来进行学习和实验;  
    2. 流程: 通过训练数据进行学习,寻找最优的参数; 然后,使用是测试数据评价训练得到的模型的实际能力;  
    3. 区分数据为训练数据和测试数据的原因: 机器学习追求的是模型的泛华能力,进行区分是为了评价模型的泛化能力;  
    4. 训练数据也可称为监督数据;  
    5. 在机器学习的过程中,在对训练数据进行学习后,也会出现模型的欠拟合和过拟合问题;
    
##### 损失函数  
* 损失函数是神经网络的学习中使用的指标-----用来寻找最优权重参数的指标  
* 常见的损失函数: 均方误差, 交叉熵误差  
* 均方误差  
    1. 公式: ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/均方误差.png)  
    其中, yk表示神经网络的输出, tk表示监督数据, k表示数据的维数;  
    2. 实现均方误差的代码  
    ```python
    def mean_squared_error(y,t):
          return 0.5 * np.sum((y-t) ** 2)
    ```  
    3. 均方误差越小,表示和监督数据之间的误差较小;  
    
* 交叉熵误差  
    1. 公式: ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/交叉熵误差.png)  
    其中, yk表示神经网络的输出, tk是正确的解标签(0/1), 因此上市只需计算正确解标签的输出的自然对数.  
    2. 实现交叉熵误差的代码  
    ```python
    def cross_entropy_error(y,t):
          delta = 1e-7
          return -np.sum(t * np.log(y + delta))
    ```   
  
* mini-batch学习  
    1. 引入mini-batch学习的原因:  
        1. 在使用损失函数的时候,我们不仅要用损失函数计算单个数据,还要对所有训练数据的损失函数求和.格式如下:  
            ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/所有训练数据的损失函数的总和.png)   
        2. 但是在处理的数据较大的情况下,想要计算所有的数据的损失函数是不现实的,因此选择一批数据,作为全部数据的"近似"  
    2. 定义: 在神经网络的学习中, 从训练数据中选出一批数据(称为mini-batch, 小批量),   
            再对每个mini-batch进行学习,这种学习方式称为mini-batch学习.  
    3. 实现mini-batch 抽取数据集的代码   
    ```python
      # 读取数据
        import sys, os sys.path.append(os.pardir)
        import numpy as np
        from dataset.mnist import load_mnist
        (x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, one_hot_label=True)
        print(x_train.shape) # (60000, 784) print(t_train.shape) # (60000, 10)
  
      # 从训练集中随机抽取数据数据(10组)
      # 通过Numpy的np.random.choice() 从训练数据中随机抽取10笔数据  
        train_size = x_train.shape[0]
        batch_size = 10
        batch_mask = np.random.choice(train_size, batch_size) 
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
    ```
* mini-batch版交叉熵误差实现

* 设定损失函数的原因  

##### 数值微分  
* 略
##### 梯度  
* 梯度法  
* 神经网络的梯度  

##### 学习算法的实现  
* 2层神经网络的类  
* mini-batch的实现  
* 基于测试数据的评价  

#### 4.误差反向传播学习法  
* 计算图  
* 链式法则  
* 反向传播  
* 简单层的实现  
* 激活函数层的实现  
* 误差反向传播算法的实现  

#### 5.卷积神经网络  
* 整体结构  
* 卷积层  
* 池化层  
* 卷积层和池化层的实现  
* CNN的实现  
* CNN的可视化  
* 代表性的CNN  

#### 6.深度学习
