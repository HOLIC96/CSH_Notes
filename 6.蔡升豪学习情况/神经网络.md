### 神经网络

#### 1.感知机
##### 感知机图示
![感知器图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/感知机图示.png)  
* 组成 : 输入层 -> 输出层  
    其中输入层为信号,从输入层到输出层需要经过权重W的组合计算  


#### 2.神经网络
##### 神经网络图示
![神经网络图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/神经网络图示.png)

##### 神经网络的组成  
* 基本组成 : 输入层 -> 隐层 -> 输出层  
    1. 其中隐层对输入层的数据进行处理,各层之间需要经过权重W和输入的计算,以及不同函数的处理;  
    2. 注意神经网络的层数由具有权重的层数决定,有几层具有权重,即神经网络的层数为几层;  
    
* 计算过程   
![计算过程图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/激活函数计算过程.png)
    1. 组合权重W和偏置B求和(以两层为例) : a = W1 * x1 + W2 * x2 + b
    2. 利用激活函数求得结果 : y = h(a)  
    
* 激活函数: h(x)  
    1. 阶跃函数
	    1. 图示  
	    ![跃迁函数图示](https://github.com/ZWtu19/Notes/blob/master/src/pics/跃迁函数.png)
	    2. 函数表达式为：h(x) =   
	    3. 在Python中的实现方式：      
    2. sigmoid函数 
	    1. 图示  
	    ![sigmoid函数](https://github.com/ZWtu19/Notes/blob/master/src/pics/sigmoid函数.png)
	    2. 函数表达式为：h(x) = 1/( 1+exp(-x) )  
	    3. 在Python中实现方式： 
        ```python
        # 其中np为Numpy, x 可以为Numpy数组
            import numpy as np
            def sigmoid(x):
                    return 1/(1+np.exp(-x))
         ```

    3. 二者异同之处:   
	    1. 共同点：宏观角度来看，形状相似，即当输入信号为重要信息时，
阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，
两者都输出较小的值；两者均为非线性函数，注意激活函数必须为非线性函数，否则神经网络分层就没有意义了；
	    2. 区别： 平滑性不同；阶跃函数只能输出0/1，sigmoid能输出连续的实数； 
    4. ReLu函数：  
    	1. 图示：  
	    2. 函数的表达式： 
	    3. 在Python中的实现方式  
	    
* 神经网络的信号传递实现三层为例)  
    1. 符号说明  
    ![权重符号说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/权重符号说明.png)    
    2. 输入层 -> 第一层  
        1. 表达式    
        a1 = w11 * x1 + w12 * x2 + b1;
        a2 = w21 * x1 + w22 * x2 + b2;  
        a3 = w31 * x1 + w22 * x2 + b3;    
        z1 = h(a1);  
        z2 = h(a2);  
        z3 = h(a3);  
        
        2. 图示说明  
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/输入层->第一层.png)
        3. 矩阵表示  
        A = XW + B  
        Z = h(A)  
        其中:  
        A = [a1, a2, a3];  
        X = [x1, x2];  
        B = [b1, b2, b3];  
        w = [[w11, w21, w31],
             [w12, w22, w32]]  
        4. 代码实现  
        ```python
            import numpy as np
            X = np.array([1.0, 0.5])
            W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) B1 = np.array([0.1, 0.2, 0.3])
            print(W1.shape) # (2, 3) print(X.shape) # (2,) print(B1.shape) # (3,)
            A1 = np.dot(X, W1) + B1
        ```  
    3. 第一层 -> 第二层  
        1. 表达式  
        a1' = w'11 * z1 + w'12 * z2 + w'13 * z3 + b'1;
        a2' = w'21 * z1 + w'22 * z2 + w'23 * z3 + b'2;      
        z1' = h'(a1);  
        z2' = h'(a2);  
         
        2. 图示说明
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/第一层->第二层.png)  
        2. 矩阵表示  
        A' = ZW' + B'  
        Z' = h'(A')
        其中:  
        A' = [a1', a2', a3'];  
        Z  = [z1, z2, z3];  
        B' = [b1', b2'];  
        W' = [[w11, w21],
             [w12, w22],
             [w13, w23]]  
          
        3. 代码实现   
        ```python
            W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
            B2 = np.array([0.1, 0.2])
            print(Z1.shape) # (3,) print(W2.shape) # (3, 2) print(B2.shape) # (2,)
            A2 = np.dot(Z1, W2) + B2 
            Z2 = sigmoid(A2)
        ```  
       
    4. 第二层 -> 输出层
        1. 表达式  
        a1'' = w'11 * z1' + w'12 * z2' b''1;  
        a2'' = w'21 * z1' + w'22 * z2' b''2;       
        y1'' = identity_function(a1'');  
        y2'' = identity_function(a2'');  
        
        2. 图示说明  
        ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/第二层->输出层.png)  
         
        3. 矩阵表示  
        A'' = ZW'' + B''  
        Z'' = identity_function(A'')
        其中:  
        A'' = [a1'', a2'', a3''];  
        Z''  = [z1', z2', z3'];  
        B'' = [b1'', b2''];  
        W'' = [[w11'', w21''],
             [w12'', w22''],
             [w13'', w23'']]  
         
        4. 代码实现  
        ```python
           def identity_function(x): return x
           W3 = np.array([[0.1, 0.3], [0.2, 0.4]])  
           B3 = np.array([0.1, 0.2])
           A3 = np.dot(Z2, W3) + B3
           Y = identity_function(A3) # 或者Y = A3                      

        ```
     
* 利用神经网络解决实际问题: 字母识别(详见....)  
    1. 收集数据集  
    从官网获取数据: http://yann.lecun.com/exdb/mnist/,下载数据  
        * train-images-idx3-ubyte.gz
        * train-labels-idx1-ubyte.gz
        * t10k-images-idx3-ubyte.gz
        * t10k-labels-idx1-ubyte.gz
        
    2. 程序实现  
    ```python
    # 导入模块
    import numpy as np
    from struct import unpack
    # 读取图片  
    def read_image(path):
      with open(path, 'rb') as f:
        magic, num, rows, cols = unpack('>4I', f.read(16))
        img = np.fromfile(f, dtype=np.uint8).reshape(num, 784)
      return img        
    # 读取标签文件  
    def read_label(path):
      with open(path, 'rb') as f:
          magic, num = unpack('>2I', f.read(8))
          lab = np.fromfile(f, dtype=np.uint8)
          return lab      
    
    # 数据处理
    # 图像的像素值正规化  
    def normalize_image(image):
      img = image.astype(np.float32) / 255.0
      return img    

    # 转换标签为one-hot编码
    def __one_hot_label(label):
        lab = np.zeros((label.size, 10))
        for i, row in enumerate(lab):
            row[label[i]] = 1
        return lab
    
    # mnist读取函数  
    def load_mnist(train_image_path, train_label_path, test_image_path, test_label_path, normalize=True, one_hot=True):
        '''读入MNIST数据集
        Parameters
        ----------
        normalize : 将图像的像素值正规化为0.0~1.0
        one_hot_label : 
            one_hot为True的情况下，标签作为one-hot数组返回
            one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组
        Returns
        ----------
        (训练图像, 训练标签), (测试图像, 测试标签)
        '''
        image = {
            'train' : __read_image(train_image_path),
            'test'  : __read_image(test_image_path)
        }
    
        label = {
            'train' : __read_label(train_label_path),
            'test'  : __read_label(test_label_path)
        }
        
        if normalize:
            for key in ('train', 'test'):
                image[key] = __normalize_image(image[key])
    
        if one_hot:
            for key in ('train', 'test'):
                label[key] = __one_hot_label(label[key])
    
        return (image['train'], label['train']), (image['test'], label['test'])

    ```      
    
#### 3.神经网络的"学习"
##### 从数据中学习  
* 数据驱动  
    1. 人工设计规则  
    2. 人工设置特征量  
    3. 机器从数据中学习  
    ![图示说明](https://github.com/ZWtu19/Notes/blob/master/src/pics/数据驱动.png)  
    4. 利用数据驱动的优点：  
	对于所有的问题都可以用同样的流程来解决；例如图像识别问题，不管是识别猫狗或是人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题。
	
* 训练数据和测试数据  
    1. 在机器学习中数据分类为训练数据和测试数据两部分来进行学习和实验;  
    2. 流程: 通过训练数据进行学习,寻找最优的参数; 然后,使用是测试数据评价训练得到的模型的实际能力;  
    3. 区分数据为训练数据和测试数据的原因: 机器学习追求的是模型的泛华能力,进行区分是为了评价模型的泛化能力;  
    4. 训练数据也可称为监督数据;  
    5. 在机器学习的过程中,在对训练数据进行学习后,也会出现模型的欠拟合和过拟合问题;
    
##### 损失函数  
* 损失函数是神经网络的学习中使用的指标-----用来寻找最优权重参数的指标  
* 常见的损失函数: 均方误差, 交叉熵误差  
* 均方误差  
    1. 公式: ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/均方误差.png)  
    其中, yk表示神经网络的输出, tk表示监督数据, k表示数据的维数;  
    2. 实现均方误差的代码  
    ```python
    def mean_squared_error(y,t):
          return 0.5 * np.sum((y-t) ** 2)
    ```  
    3. 均方误差越小,表示和监督数据之间的误差较小;  
    
* 交叉熵误差  
    1. 公式: ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/交叉熵误差.png)  
    其中, yk表示神经网络的输出, tk是正确的解标签(0/1), 因此上市只需计算正确解标签的输出的自然对数.  
    2. 实现交叉熵误差的代码  
    ```python
    def cross_entropy_error(y,t):
          delta = 1e-7
          return -np.sum(t * np.log(y + delta))
    ```   
  
* mini-batch学习  
    1. 引入mini-batch学习的原因:  
        1. 在使用损失函数的时候,我们不仅要用损失函数计算单个数据,还要对所有训练数据的损失函数求和.格式如下:  
            ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/所有训练数据的损失函数的总和.png)   
        2. 但是在处理的数据较大的情况下,想要计算所有的数据的损失函数是不现实的,因此选择一批数据,作为全部数据的"近似"  
    2. 定义: 在神经网络的学习中, 从训练数据中选出一批数据(称为mini-batch, 小批量),   
            再对每个mini-batch进行学习,这种学习方式称为mini-batch学习.  
    3. 实现mini-batch 抽取数据集的代码   
    ```python
      # 读取数据
        import sys, os sys.path.append(os.pardir)
        import numpy as np
        from dataset.mnist import load_mnist
        (x_train, t_train), (x_test, t_test) = \ load_mnist(normalize=True, one_hot_label=True)
        print(x_train.shape) # (60000, 784) print(t_train.shape) # (60000, 10)
  
      # 从训练集中随机抽取数据数据(10组)
      # 通过Numpy的np.random.choice() 从训练数据中随机抽取10笔数据  
        train_size = x_train.shape[0]
        batch_size = 10
        batch_mask = np.random.choice(train_size, batch_size) 
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
    ```
* mini-batch版交叉熵误差实现

* 设定损失函数的原因  
    1. 要求 : 在进行神经网络的学习时, 不能识别精度作为指标, 因为如果以识别精度作为指标,则参数的导数在绝大多数地方都会变为0;  
    2. 为什么不适用识别精度作为指标: 
        在神经网络学习中,寻找最优参数(权重和偏置)的时候,往往是通过计算参数的导数(梯度),然后以此作为指引,逐步更新参数的值.  
        而在使用识别精度作为指标时,通过求导,权重参数的更新会停在导数为0的地方,导致参数无法更新.
    3. 为什么使用损失函数作为指标:  
        同上,在使用损失函数作为指标的时候, 稍微改变参数的值,  
        对应的损失函数会发生连续的变化,  
        而不是像阶跃函数或以识别精度作为指标的函数一样是不连续的. 
    
##### 数值微分  
* 理解 : 略   
* 在python代码中的实现 :  
    1. 函数  
    ```python
  # 实现 y = 0.01 X*X + 0.1 * X 函数
  def function_1(x): 
      return 0.01 * x ** 2 + 0.1 * x
    ```  
  
    2. 导数  
    ```python

  # 通过导数的定义求得导数的值
  # 请注意定义的h是很小的值,所以求得的值可以看成近似等于导数值
  def numerical_diff(f,x):  
    h = 1e-4
    return (f(x+h) - f(x-h)) / (2 * h)
    ```  
  
   3. 偏导数  举例如下:  
   ![均方误差](https://github.com/ZWtu19/Notes/blob/master/src/pics/偏导数求导.png)  
   在求偏导的时候,对于另一个变量,可以当成常量计算.  
   
##### 梯度
* 梯度的概念  
    举例,在函数f(x0,x1)中, 求在x0= 3, x1=4时(x0, x1)的偏导数(f'x0, f'y0),  
    求得的向量为梯度.  
    
* 梯度的实现代码  
    ```python
  import numpy as np
  def numerical_gradient(f, x):  
    h = 1e-4 # 0.0001  
    grad = np.zeros_like(x) 
  
    for idx in range(x.size):
      tmp_val = x[idx]
      # 计算f(x+h)
      x[idx] = tmp_val + h 
      fxh1 = f(x)
  
      # 计算f(x-h)
      x[idx] = tmp_val - h 
      fxh2 = f(x)
  
      # 通过定义法求得梯度  
      x[idx] = tmp_val # 还原值
  
    return grad 
    ```  
* f(x0, x1) = x0 ** 2 + x1 ** 2的梯度图  
    ![梯度示意图](https://github.com/ZWtu19/Notes/blob/master/src/pics/梯度举例图.png)  
    理解: 实际上, 梯度作为向量,可以视为"箭头",而梯度指示的方向是各点处的函数值减少最多的方向.  
  
* 梯度法  
    1. 为什么使用梯度法  
        机器学习的主要任务是在学习时寻找最优参数; 故神经网络也必须在学习时找到最优参数(权重和偏置);  
        最优参数是指损失函数取最小值时的参数;  
        针对复杂的损失函数, 我们通过梯度法来寻找函数的最小值.  
        
    2. 使用梯度法的数学式,见下图  
        ![数学式示意图](https://github.com/ZWtu19/Notes/blob/master/src/pics/数学式.png)  
        其中, n表示更新量,称为学习率;上述式子表示更新一次,将会反复执行此步骤,逐渐减少函数值;  
        在梯度法中,  
        函数的取值从当前位置沿着梯度方向前进一定距离,然后在新的地方求梯度,再沿着新梯度方向前进,如此反复,不断地沿梯度方向前进;  
        根据寻找最大/小值的梯度法为梯度下降/上升法;  
        梯度的方向能够最大限度地减少函数的值. 因此,在寻找函数的最小值的位置的任务中,要以梯度的信息为线索,决定前进的方向   
        
    3. 梯度法的执行过程    
        1. 学习率先确定为某个值
        2. 实现梯度下降  
        代码实现如下:  
        ```python
       def gradient_descent(f, init_x, lr=0.01, step_num=100):
           x = init_x
           for i in range(step_num):
               grad = numerical_gradient(f, x) 
               x -= lr * grad
           return x
        ```  
        3. 举例: 用梯度法求函数的最小值  
            见Projects/1.神经网络/Codes/MinInt.py
        
* 神经网络的梯度  
    1. 神经网络的学习也要求梯度,其梯度是指损失函数关于权重参数的梯度.  
    2. 以简单的神经网络为例,来实现求梯度的代码.  
        见Projects/1.神经网络/Codes/grad.py
 
##### 学习算法的实现  
神经网络的学习步骤总结  
    1. 前提  
        神经网络存在合适的权重和偏置,调整权重和偏置以便拟合训练数据的过程称为'学习'.  
        神经网络的学习分成以下4个步骤;  
    2. 步骤一(mini-batch)  
        从训练数据中随机选出一部分数据,这部分数据称为mini-batch.   
        我们的目标是减少mini-batch的损失函数的值;  
    3. 步骤二  
        为了减少mini-batch的损失函数的值,需要求出各个权重参数的梯度.  
        梯度表示损失函数的值较少最多的方向.  
    4. 步骤三  
        将权重参数研梯度方向进行微小更新;  
    5. 步骤四  
        重复步骤一,步骤二,步骤三  
    个人心得 : 以上就是神经网络学习过程的实现步骤.  
    由于mini-batch是随机选择的,所以称为随机梯度下降法;  
    以下将用mnist数据集进行学习  
    详见 Projects/1.神经网络/Codes/1.神经网络算法的实现 
* 2层神经网络的类  
    1. 定义TwolayerNet类,其组成结构如下:  
    其中定义的方法如下:  
    |方法|说明|
    |---|---|
    |__init__(self, input_size, hidden_size, output_size, weight_init_std=0.01)|进行初始化。 参数从头开始依次表示输入层的神经元数、隐藏层 的神经元数、输出层的神经元数|
    |predict(self,x)|进行识别(推理)。 参数 x 是图像数据|  
    |loss(self, x, t)|计算损失函数的值。参数 x 是图像数据，t 是正确解标签(后面 3 个方法 的参数也一样)|  
    |accuracy(self, x, t)|计算识别精度|  
    |numerical_gradient(self, x, t)|计算权重参数的梯度|  
    |gradient(self, x, t)|计算权重参数的梯度。numerical_gradient() 的高速版，将在下一章实现|
    其中,该类中,有params和grads两个字典型实例变量,分别用于保存权重参数和偏置;  
    __init__() 函数通过高斯分布(高斯分布链接)的随机数进行初始化;  
    predict() 函数 和 accuracy() 实现神经网络的推理处理;  
    loss() 函数用于计算损失损失函数值,该方法会基于predict() 的结果和正确解标签计算交叉熵误差;  
    numerical_gradient(self, x, t) 函数计算各个参数的梯度,计算各个参数相对于损失函数的梯度;  
    gradient(self, x, t) 函数使用误差反向传播法计算梯度;  
* mini-batch的实现  
* 基于测试数据的评价  

#### 4.误差反向传播学习法  
* 计算图  
* 链式法则  
* 反向传播  
* 简单层的实现  
* 激活函数层的实现  
* 误差反向传播算法的实现  

#### 5.卷积神经网络  
* 整体结构  
* 卷积层  
* 池化层  
* 卷积层和池化层的实现  
* CNN的实现  
* CNN的可视化  
* 代表性的CNN  

#### 6.深度学习
